<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ffpangestu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ffpangestu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-09T14:01:09+00:00</updated><id>https://ffpangestu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Shopkeeper’s Journey in the Simulated Environment</title><link href="https://ffpangestu.github.io/blog/2025/shopkeepers-playground/" rel="alternate" type="text/html" title="Shopkeeper’s Journey in the Simulated Environment"/><published>2025-09-29T00:00:00+00:00</published><updated>2025-09-29T00:00:00+00:00</updated><id>https://ffpangestu.github.io/blog/2025/shopkeepers-playground</id><content type="html" xml:base="https://ffpangestu.github.io/blog/2025/shopkeepers-playground/"><![CDATA[<p>In this little project, we take a closer look at the transformation of a <strong>simple shop</strong> into a <strong>warehouse environment</strong>.<br/> Using <strong>OpenAI Gymnasium</strong> as the base of our Reinforcement Learning (RL) environment, we can observe how small design tweaks completely shift the agent’s behavior and strategy.</p> <p>Our agent relies only on a <strong>simple tabular Q-learning algorithm</strong> — no deep learning, no tricks.<br/> The rule of the game is simple:</p> <blockquote> <p><strong>Whatever we do, never increase the product price.</strong></p> </blockquote> <hr/> <h2 id="the-opening-day">The Opening Day</h2> <p>A new tiny shop opened in the digital world! The humble shop ran a small apple stand with a <strong>maximum inventory of 10 apples</strong>. At the start, it only had <strong>5 apples</strong> and <strong>€5 cash</strong> in hand.</p> <p>To make things more interesting, the contract only lasted <strong>20 days</strong> (<code class="language-plaintext highlighter-rouge">step: 20</code>). The mechanics were straightfoward:</p> <ul> <li>The shopkeeper (agent) could <strong>either sell or restock</strong> in each step.</li> <li>The customer could buy <strong>1–3 apples</strong> per step.</li> <li><strong>Selling price:</strong> €3 per apple</li> <li><strong>Restock cost:</strong> €1 per apple</li> <li><strong>Unfulfilled demand penalty:</strong> –€2 per apple</li> </ul> <p>So, the agent had to balance between keeping stock high enough to meet demand and saving cash to avoid unnecessary restocks.</p> <p>Now for reward and penalties. Because this was the shopkeeper’s first job, he took customer satisfaction seriously. Whenever he couldn’t meet the customer’s demand, he felt guilty — which translated into penalties in our reward system:</p> <ul> <li><strong>Successful sale:</strong> +1</li> <li><strong>Failing to meet demand:</strong> –3</li> </ul> <p>These values shaped how the agent learned what “good” and “bad” decisions looked like over time.</p> <p>With basic Q-learning parameters, the agent ended the simulation with an <strong>average cash of €169.86</strong>. Not bad for the first time, right?</p> <hr/> <h2 id="expanding-demand">Expanding Demand</h2> <p>Encouraged by his success, the shopkeeper expanded his apple stand operations. The contract was now extended to a <strong>full year</strong> (<code class="language-plaintext highlighter-rouge">step: 20 → 365</code>), and new customers began to appear. Feeling confident, the shopkeeper agreed to <strong>accept larger orders</strong> — now the customer could buy the <strong>entire stock</strong>! (<code class="language-plaintext highlighter-rouge">max_demand: 3 → max_inventory</code>)</p> <p>The result? This simple rule change completely altered the agent’s behavior. Because failing to meet big orders felt much worse than before, the shopkeeper started to <strong>hoard apples</strong> — saving them up for large buyers instead of selling them all at once.</p> <p>The new setup led to a dramatic jump in profit. The agent achieved an <strong>average cash of €734.33</strong>, showing that even <strong>small changes in environment dynamics</strong> can have <strong>major effects on RL strategies</strong>.</p> <hr/> <h2 id="warehouse-for-lease">Warehouse for Lease</h2> <p>Bigger orders called for a <strong>bigger inventory</strong> — and so the next logical step was to <strong>lease an entire warehouse</strong>. The maximum inventory increased dramatically from <strong>10 → 100</strong>. However, this upgrade came with a new challenge: every stored unit now incurred a <strong>storage fee</strong>, charged <strong>every step</strong>. In other words, holding too much stock could quietly drain the cash reserves.</p> <p>This is where things got <strong>interesting (and messy)</strong>. With the new cost structure, the <strong>reward function failed to converge</strong>. The agent simply couldn’t stabilize its learning process.</p> <p>Instead of growing profits, the <strong>cash balance collapsed</strong> to <strong>€561.86</strong>! The shopkeeper couldn’t figure out the right balance between:</p> <ul> <li>keeping enough stock to meet big orders, and</li> <li>minimizing storage costs that grew with every apple on the shelf.</li> </ul> <p>Realizing something was wrong, the shopkeeper decided to <strong>train longer</strong> for a whopping <strong>100,000 episodes</strong>. This extended training period helped the agent adapt to the new, more complex environment.</p> <p>The result? <strong>Average cash improved</strong> to <strong>€847.55</strong>, but the <strong>reward values remained negative</strong>. Even worse, the agent only used <strong>30% of the available inventory</strong>, meaning it was still <strong>leaving a lot of money on the table</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/0.2.1-480.webp 480w,/assets/img/0.2.1-800.webp 800w,/assets/img/0.2.1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/0.2.1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/>]]></content><author><name>Firstka Faiz Pangestu</name></author><category term="gymnasium"/><category term="RL"/><category term="environment"/><category term="python"/><category term="qlearning"/><summary type="html"><![CDATA[A humble beginning for a humble RL-environment]]></summary></entry></feed>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ffpangestu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ffpangestu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-09T16:55:04+00:00</updated><id>https://ffpangestu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Shopkeeper’s Journey in the Simulated Environment</title><link href="https://ffpangestu.github.io/blog/2025/shopkeepers-playground/" rel="alternate" type="text/html" title="Shopkeeper’s Journey in the Simulated Environment"/><published>2025-09-29T00:00:00+00:00</published><updated>2025-09-29T00:00:00+00:00</updated><id>https://ffpangestu.github.io/blog/2025/shopkeepers-playground</id><content type="html" xml:base="https://ffpangestu.github.io/blog/2025/shopkeepers-playground/"><![CDATA[<p>In this little project, we take a closer look at the transformation of a <strong>simple shop</strong> into a <strong>warehouse environment</strong>. Using <strong>OpenAI Gymnasium</strong> as the base of our Reinforcement Learning (RL) environment, we can observe how small design tweaks completely shift the agent’s behavior and strategy.</p> <p>Our agent relies only on a <strong>simple tabular Q-learning algorithm</strong> — no deep learning, no tricks. The rule of the game is simple:</p> <blockquote> <p><strong>Whatever we do, never increase the product price.</strong></p> </blockquote> <hr/> <h2 id="the-opening-day">The Opening Day</h2> <p>A new tiny shop opened in the digital world! The humble shop ran a small apple stand with a <strong>maximum inventory of 10 apples</strong>. At the start, it only had <strong>5 apples</strong> and <strong>€5 cash</strong> in hand.</p> <p>To make things more interesting, the contract only lasted <strong>20 days</strong> (<code class="language-plaintext highlighter-rouge">step: 20</code>). The mechanics were straightfoward:</p> <ul> <li>The shopkeeper (agent) could <strong>either sell or restock</strong> in each step.</li> <li>The customer could buy <strong>1–3 apples</strong> per step.</li> <li><strong>Selling price:</strong> €3 per apple</li> <li><strong>Restock cost:</strong> €1 per apple</li> <li><strong>Unfulfilled demand penalty:</strong> –€2 per apple</li> </ul> <p>So, the agent had to balance between keeping stock high enough to meet demand and saving cash to avoid unnecessary restocks.</p> <p>Now for reward and penalties. Because this was the shopkeeper’s first job, he took customer satisfaction seriously. Whenever he couldn’t meet the customer’s demand, he felt guilty — which translated into penalties in our reward system:</p> <ul> <li><strong>Successful sale:</strong> +1</li> <li><strong>Failing to meet demand:</strong> –3</li> </ul> <p>These values shaped how the agent learned what “good” and “bad” decisions looked like over time.</p> <p>With basic Q-learning parameters, the agent ended the simulation with an <strong>average cash of €169.86</strong>. Not bad for the first time, right?</p> <hr/> <h2 id="expanding-demand">Expanding Demand</h2> <p>Encouraged by his success, the shopkeeper expanded his apple stand operations. The contract was now extended to a <strong>full year</strong> (<code class="language-plaintext highlighter-rouge">step: 20 → 365</code>), and new customers began to appear. Feeling confident, the shopkeeper agreed to <strong>accept larger orders</strong> — now the customer could buy the <strong>entire stock</strong>! (<code class="language-plaintext highlighter-rouge">max_demand: 3 → max_inventory</code>)</p> <p>The result? This simple rule change completely altered the agent’s behavior. Because failing to meet big orders felt much worse than before, the shopkeeper started to <strong>hoard apples</strong> — saving them up for large buyers instead of selling them all at once.</p> <p>The new setup led to a dramatic jump in profit. The agent achieved an <strong>average cash of €734.33</strong>, showing that even <strong>small changes in environment dynamics</strong> can have <strong>major effects on RL strategies</strong>.</p> <hr/> <h2 id="warehouse-for-lease">Warehouse for Lease</h2> <p>Bigger orders called for a <strong>bigger inventory</strong> — and so the next logical step was to <strong>lease an entire warehouse</strong>. The maximum inventory increased dramatically from <strong>10 → 100</strong>. However, this upgrade came with a new challenge: every stored unit now incurred a <strong>storage fee</strong>, charged <strong>every step</strong>. In other words, holding too much stock could quietly drain the cash reserves.</p> <p>This is where things got <strong>interesting (and messy)</strong>. With the new cost structure, the <strong>reward function failed to converge</strong>. The agent simply couldn’t stabilize its learning process.</p> <p>Instead of growing profits, the <strong>cash balance collapsed</strong> to <strong>€561.86</strong>! The shopkeeper couldn’t figure out the right balance between:</p> <ul> <li>keeping enough stock to meet big orders, and</li> <li>minimizing storage costs that grew with every apple on the shelf.</li> </ul> <p>Realizing something was wrong, the shopkeeper decided to <strong>train longer</strong> for a whopping <strong>100,000 episodes</strong>. This extended training period helped the agent adapt to the new, more complex environment.</p> <p>The result? <strong>Average cash improved</strong> to <strong>€847.55</strong>, but the <strong>reward values remained negative</strong>. Even worse, the agent only used <strong>30% of the available inventory</strong>, meaning it was still <strong>leaving a lot of money on the table</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/0.2.1-480.webp 480w,/assets/img/0.2.1-800.webp 800w,/assets/img/0.2.1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/0.2.1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The stock balance and cash growth. </div> <hr/> <h2 id="helpers-for-hire">Helpers for Hire</h2> <p>It soon became clear that with a bigger warehouse, the shopkeeper couldn’t manage everything alone. To keep up with demand, he decided to <strong>hire helpers</strong>. These helpers <strong>multiplied the restocking rate</strong>, allowing the warehouse to refill much faster, but they also came with a price: <strong>€1 per helper per step</strong>.</p> <p>To manage them, the shopkeeper gained <strong>two new actions</strong>:</p> <ul> <li><strong>Hire a helper</strong> (up to 5 total)</li> <li><strong>Fire a helper</strong></li> </ul> <p>The first results were a disaster. The agent went wild, hiring and firing helpers recklessly without considering the overall cash flow. As a result, the <strong>average cash plummeted to –€480</strong>. The warehouse became a financial mess but full of apples.</p> <p>To encourage smarter management, a <strong>penalty for firing workers (–5)</strong> was introduced. This change nudged the agent to retain helpers longer and think twice before letting them go. Then came another training session, this time for <strong>200,000 episodes</strong>. Finally, it clicked.</p> <p>The result was remarkable: <strong>Average cash soared to €1,369.</strong> The shopkeeper had finally mastered the art of <strong>balancing workforce, stock, and cost</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/0.3.4-480.webp 480w,/assets/img/0.3.4-800.webp 800w,/assets/img/0.3.4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/0.3.4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Maximizing helpers effectiveness. </div> <hr/> <h2 id="customer-behavior">Customer Behavior</h2> <p>After a long time in business, the shopkeeper began to <strong>notice something curious</strong>: customer demand wasn’t completely random. It seemed to follow a <strong>pattern</strong>, more like a <strong>normal distribution</strong> than uniform chaos.</p> <p>To capture this insight, the environment was updated to model demand accordingly:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">base</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_min_demand</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">_max_demand</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">self</span><span class="p">.</span><span class="n">customer</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">np_random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">max_step</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">customer</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">customer</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">_min_demand</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">_max_demand</span><span class="p">)</span>
</code></pre></div></div> <p>The results produced the best performance: <strong>€1,540</strong> average cash.</p> <hr/> <h2 id="lesson-learned">Lesson Learned</h2> <p>After several iterations, experiments, and plenty of failed training runs, the shopkeeper’s journey revealed a few key lessons about reinforcement learning design:</p> <ol> <li> <p><strong>Reward shaping can completely change agent behavior.</strong><br/> Even a small adjustment in rewards or penalties can flip the agent’s entire strategy, From cautious hoarding to aggressive selling. Reward functions should always align closely with the true goals of the environment.</p> </li> <li> <p><strong>Add complexity carefully.</strong><br/> Each new mechanic (storage costs, helpers, or customer demand) initially <em>breaks</em> the agent. When the environment changes, the learning dynamics must be rebalanced through parameter tuning, normalization, or new reward terms.</p> </li> <li> <p><strong>Reducing randomness improves learning.</strong><br/> By modeling customer demand using a <strong>normal distribution</strong> instead of uniform randomness, the agent’s decisions became more stable and interpretable. A more predictable environment helps the agent learn patterns instead of chasing noise.</p> </li> </ol> <hr/>]]></content><author><name>Firstka Faiz Pangestu</name></author><category term="gymnasium"/><category term="RL"/><category term="environment"/><category term="python"/><category term="qlearning"/><summary type="html"><![CDATA[A humble beginning for a humble RL-environment]]></summary></entry></feed>